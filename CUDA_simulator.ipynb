{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMoYYtHV1picGPxA3m6rrjV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshulsawant/llm-systems/blob/main/CUDA_simulator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ovnlWerSVhG5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['NUMBA_ENABLE_CUDASIM'] = '1'\n",
        "import numba\n",
        "import numpy as np\n",
        "from numba import cuda"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@cuda.jit\n",
        "def vec_add(A, B, n, out):\n",
        "    x = cuda.threadIdx.x\n",
        "    bx = cuda.blockIdx.x\n",
        "    bdx = cuda.blockDim.x\n",
        "    i = bx * bdx + x\n",
        "    if i < n:\n",
        "      out[i] = A[i] + B[i]"
      ],
      "metadata": {
        "id": "vVDLI42_VpCe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 14\n",
        "A = np.arange(n)\n",
        "B = np.ones_like(A)\n",
        "C = np.zeros_like(A)"
      ],
      "metadata": {
        "id": "8p4N4kZlWYMP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "griddim = 4\n",
        "blockdim = 4\n",
        "vec_add[griddim, blockdim](A, B, n, C)\n",
        "C"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1q6ukcbXk-M",
        "outputId": "514d2d4d-09ab-4a90-9a17-fb8be6a42bef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@cuda.jit(device=True)\n",
        "def index_to_position(index, strides, num_dims):\n",
        "    '''\n",
        "     Converts a multidimensional tensor index into a single-dimensional position in storage\n",
        "     based on strides.\n",
        "     Args:\n",
        "        index: index tuple of ints\n",
        "        strides: tensor strides\n",
        "        num_dims: number of dimensions in the tensor, e.g. shape/strides of [2, 3, 4] has 3 dimensions\n",
        "\n",
        "     Returns:\n",
        "        int - position in storage\n",
        "    '''\n",
        "    position = 0;\n",
        "    for i in range(num_dims):\n",
        "        position += index[i] * strides[i];\n",
        "    return position;\n",
        "\n",
        "@cuda.jit(device=True)\n",
        "def to_index(ordinal, shape, out_index, num_dims):\n",
        "    '''\n",
        "     Convert an ordinal to an index in the shape. Should ensure that enumerating position 0 ... size of\n",
        "     a tensor produces every index exactly once. It may not be the inverse of index_to_position.\n",
        "     Args:\n",
        "        ordinal: ordinal position to convert\n",
        "        shape: tensor shape\n",
        "        out_index: return index corresponding to position\n",
        "        num_dims: number of dimensions in the tensor\n",
        "\n",
        "     Returns:\n",
        "        None (Fills in out_index)\n",
        "    '''\n",
        "    cur_ord = ordinal;\n",
        "    for i in reversed(range(num_dims)):\n",
        "        sh = shape[i];\n",
        "        out_index[i] = cur_ord % sh;\n",
        "        cur_ord //= sh;\n",
        "\n",
        "@cuda.jit(device=True)\n",
        "def broadcast_index(big_index, big_shape, shape, out_index, num_dims_big, num_dims):\n",
        "    '''\n",
        "     Convert a big_index into big_shape to a smaller out_index into shape following broadcasting rules.\n",
        "     In this case it may be larger or with more dimensions than the shape given.\n",
        "     Additional dimensions may need to be mapped to 0 or removed.\n",
        "\n",
        "     Args:\n",
        "        big_index: multidimensional index of bigger tensor\n",
        "        big_shape: tensor shape of bigger tensor\n",
        "        nums_big_dims: number of dimensions in bigger tensor\n",
        "        out_index: multidimensional index of smaller tensor\n",
        "        shape: tensor shape of smaller tensor\n",
        "        num_dims: number of dimensions in smaller tensor\n",
        "\n",
        "     Returns:\n",
        "        None (Fills in out_index)\n",
        "    '''\n",
        "    for i in range(num_dims):\n",
        "        if shape[i] > 1:\n",
        "            out_index[i] = big_index[i + (num_dims_big - num_dims)]\n",
        "        else:\n",
        "            out_index[i] = 0\n"
      ],
      "metadata": {
        "id": "2iBo38geZqsV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@cuda.jit('void(float32[:], int32[:], int32[:], float32[:], int32[:], int32[:], float32[:], int32[:], int32[:])')\n",
        "def simple_matmul(out, out_shape, out_strides, a, a_shape, a_strides, b, b_shape, b_strides):\n",
        "  '''\n",
        "  a: M X H\n",
        "  b: H x N\n",
        "  c: M x N\n",
        "  all shapes and strides are 2-tuples\n",
        "  '''\n",
        "  sz = np.dtype(np.float32).itemsize\n",
        "  x = cuda.threadIdx.x\n",
        "  bx = cuda.blockIdx.x\n",
        "  bd = cuda.blockDim.x\n",
        "\n",
        "  out_ordinal = bd * bx + x\n",
        "  out_index = [0]*2\n",
        "  to_index(out_ordinal, out_shape, out_index, 2)\n",
        "  if out_index[0] >= out_shape[0] or out_index[1] >= out_shape[1]:\n",
        "    return\n",
        "  out_pos = int(index_to_position(out_index, out_strides, 2))\n",
        "  out[out_pos//sz] = 0\n",
        "  for k in range(a_shape[1]):\n",
        "    a_index = [out_index[0], k]\n",
        "    b_index = [k, out_index[1]]\n",
        "    a_pos = int(index_to_position(a_index, a_strides, 2))\n",
        "    b_pos = int(index_to_position(b_index, b_strides, 2))\n",
        "    out[out_pos//sz] += a[a_pos//sz] * b[b_pos//sz]\n",
        "  cuda.syncthreads()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "id": "e0l_YJbempgR",
        "outputId": "ae85bcff-3c22-4d4a-b737-b94e284fe6ea"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "CudaSupportError",
          "evalue": "Error at driver init: \n\nCUDA driver library cannot be found.\nIf you are sure that a CUDA driver is installed,\ntry setting environment variable NUMBA_CUDA_DRIVER\nwith the file path of the CUDA driver shared library.\n:",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCudaSupportError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-9b687c16ad8d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'void(float32[:], int32[:], int32[:], float32[:], int32[:], int32[:], float32[:], int32[:], int32[:])'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msimple_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_strides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_strides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_strides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   '''\n\u001b[1;32m      4\u001b[0m   \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mM\u001b[0m \u001b[0mX\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mH\u001b[0m \u001b[0mx\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/decorators.py\u001b[0m in \u001b[0;36m_jit\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    132\u001b[0m                         \u001b[0mdisp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                     \u001b[0mdisp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mdisp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_specialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspecialized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/dispatcher.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, sig)\u001b[0m\n\u001b[1;32m    930\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Compilation disabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m             \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargetoptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m             \u001b[0;31m# We call bind to force codegen, so that there is a cubin to cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numba/core/compiler_lock.py\u001b[0m in \u001b[0;36m_acquire_compile_lock\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_acquire_compile_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_acquire_compile_lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/dispatcher.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, py_func, argtypes, link, debug, lineinfo, inline, fastmath, extensions, max_registers, opt, device)\u001b[0m\n\u001b[1;32m     80\u001b[0m         }\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_current_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_capability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         cres = compile_cuda(self.py_func, types.void, self.argtypes,\n\u001b[1;32m     84\u001b[0m                             \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/api.py\u001b[0m in \u001b[0;36mget_current_device\u001b[0;34m()\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_current_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;34m\"Get current device associated with the current thread\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcurrent_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/cudadrv/devices.py\u001b[0m in \u001b[0;36mget_context\u001b[0;34m(devnum)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCUDA\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \"\"\"\n\u001b[0;32m--> 220\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_runtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_or_create_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/cudadrv/devices.py\u001b[0m in \u001b[0;36mget_or_create_context\u001b[0;34m(self, devnum)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mattached_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_attached_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mattached_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_or_create_context_uncached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mattached_ctx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/cudadrv/devices.py\u001b[0m in \u001b[0;36m_get_or_create_context_uncached\u001b[0;34m(self, devnum)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;31m# Try to get the active context in the CUDA stack or\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;31m# activate GPU-0 with the primary context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_active_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mac\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mac\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activate_context_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m                 \u001b[0mhctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrvapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcu_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m                 \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuCtxGetCurrent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m                 \u001b[0mhctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhctx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, fname)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialization_error\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m             raise CudaSupportError(\"Error at driver init: \\n%s:\" %\n\u001b[0m\u001b[1;32m    273\u001b[0m                                    self.initialization_error)\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCudaSupportError\u001b[0m: Error at driver init: \n\nCUDA driver library cannot be found.\nIf you are sure that a CUDA driver is installed,\ntry setting environment variable NUMBA_CUDA_DRIVER\nwith the file path of the CUDA driver shared library.\n:",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@cuda.jit('void(float32[:], int32[:], int32[:], float32[:], int32[:], int32[:], float32[:], int32[:], int32[:], int32)')\n",
        "def simple_batch_matmul(out, out_shape, out_strides, a, a_shape, a_strides, b, b_shape, b_strides):\n",
        "  '''\n",
        "  a: B X M X H\n",
        "  b: B X H x N\n",
        "  c: B X M x N\n",
        "  all shapes and strides are 3-tuples\n",
        "  '''\n",
        "  sz = np.dtype(np.float32).itemsize\n",
        "  num_dims = 3\n",
        "\n",
        "  _a = cuda.shared.array((blockDim.x, blockDim.y))\n",
        "  _b = cuda.shared.array((blockDim.x, blockDim.y))\n",
        "\n",
        "  ## Each block is threads_per_block x threads_per_block\n",
        "  ## Each grid is ceil(M/threads_per_block) x ceil(N/threads_per_block) x B\n",
        "\n",
        "  batch = cuda.blockIdx.z\n",
        "  if batch >= out_shape[0]:\n",
        "    return\n",
        "  out_x = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
        "  out_y = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n",
        "\n",
        "  out_index = [batch, out_x, out_y]\n",
        "  if out_index[1] >= out_shape[1] or out_index[2] >= out_shape[2]:\n",
        "    return\n",
        "  out_pos = int(index_to_position(out_index, out_strides, num_dims))\n",
        "  out[out_pos//sz] = 0\n",
        "  for k in range(a_shape[2]):\n",
        "    a_index = [batch, out_index[1], k]\n",
        "    b_index = [batch, k, out_index[2]]\n",
        "    a_pos = int(index_to_position(a_index, a_strides, num_dims))\n",
        "    b_pos = int(index_to_position(b_index, b_strides, num_dims))\n",
        "    out[out_pos//sz] += a[a_pos//sz] * b[b_pos//sz]\n",
        "  cuda.syncthreads()"
      ],
      "metadata": {
        "id": "7YN5WB5BDhBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1\n",
        "M = 3\n",
        "N = 2\n",
        "H = 4\n",
        "threads_per_block = 2\n",
        "A = np.random.randn(batch_size, M, H).astype(np.float32)\n",
        "B = np.random.randn(batch_size, H, N).astype(np.float32)\n",
        "C = np.zeros((batch_size, M, N), dtype=np.float32)\n",
        "gridDim = ((M + threads_per_block - 1)//threads_per_block, (N + threads_per_block - 1)//threads_per_block, batch_size)\n",
        "blockDim = (threads_per_block, threads_per_block)\n",
        "C_np = A @ B\n",
        "grid_dim = (int(), int(), 1)\n",
        "simple_batch_matmul[gridDim, blockDim](C.reshape(-1), C.shape, C.strides, A.reshape(-1), A.shape, A.strides, B.reshape(-1), B.shape, B.strides)\n",
        "\n",
        "assert np.allclose(C, C_np)"
      ],
      "metadata": {
        "id": "bviPhF1Yr4Su"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}