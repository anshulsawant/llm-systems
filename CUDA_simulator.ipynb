{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8+DtLAmc+V3GjUsa8h/Zd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshulsawant/llm-systems/blob/main/CUDA_simulator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "ovnlWerSVhG5"
      },
      "outputs": [],
      "source": [
        "import numba\n",
        "import os\n",
        "os.environ['NUMBA_ENABLE_CUDASIM'] = '1'\n",
        "import numpy as np\n",
        "from numba import cuda"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@cuda.jit\n",
        "def vec_add(A, B, n, out):\n",
        "    x = cuda.threadIdx.x\n",
        "    bx = cuda.blockIdx.x\n",
        "    bdx = cuda.blockDim.x\n",
        "    i = bx * bdx + x\n",
        "    if i < n:\n",
        "      out[i] = A[i] + B[i]"
      ],
      "metadata": {
        "id": "vVDLI42_VpCe"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 14\n",
        "A = np.arange(n)\n",
        "B = np.ones_like(A)\n",
        "C = np.zeros_like(A)"
      ],
      "metadata": {
        "id": "8p4N4kZlWYMP"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "griddim = 4\n",
        "blockdim = 4\n",
        "vec_add[griddim, blockdim](A, B, n, C)\n",
        "C"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1q6ukcbXk-M",
        "outputId": "964f3e65-289a-4a42-9048-962e4d9b9ce0"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@cuda.jit(device=True)\n",
        "def index_to_position(index, strides, num_dims):\n",
        "    '''\n",
        "     Converts a multidimensional tensor index into a single-dimensional position in storage\n",
        "     based on strides.\n",
        "     Args:\n",
        "        index: index tuple of ints\n",
        "        strides: tensor strides\n",
        "        num_dims: number of dimensions in the tensor, e.g. shape/strides of [2, 3, 4] has 3 dimensions\n",
        "\n",
        "     Returns:\n",
        "        int - position in storage\n",
        "    '''\n",
        "    position = 0;\n",
        "    for i in range(num_dims):\n",
        "        position += index[i] * strides[i];\n",
        "    return position;\n",
        "\n",
        "@cuda.jit(device=True)\n",
        "def to_index(ordinal, shape, out_index, num_dims):\n",
        "    '''\n",
        "     Convert an ordinal to an index in the shape. Should ensure that enumerating position 0 ... size of\n",
        "     a tensor produces every index exactly once. It may not be the inverse of index_to_position.\n",
        "     Args:\n",
        "        ordinal: ordinal position to convert\n",
        "        shape: tensor shape\n",
        "        out_index: return index corresponding to position\n",
        "        num_dims: number of dimensions in the tensor\n",
        "\n",
        "     Returns:\n",
        "        None (Fills in out_index)\n",
        "    '''\n",
        "    cur_ord = ordinal;\n",
        "    for i in reversed(range(num_dims)):\n",
        "        sh = shape[i];\n",
        "        out_index[i] = cur_ord % sh;\n",
        "        cur_ord //= sh;\n",
        "\n",
        "@cuda.jit(device=True)\n",
        "def broadcast_index(big_index, big_shape, shape, out_index, num_dims_big, num_dims):\n",
        "    '''\n",
        "     Convert a big_index into big_shape to a smaller out_index into shape following broadcasting rules.\n",
        "     In this case it may be larger or with more dimensions than the shape given.\n",
        "     Additional dimensions may need to be mapped to 0 or removed.\n",
        "\n",
        "     Args:\n",
        "        big_index: multidimensional index of bigger tensor\n",
        "        big_shape: tensor shape of bigger tensor\n",
        "        nums_big_dims: number of dimensions in bigger tensor\n",
        "        out_index: multidimensional index of smaller tensor\n",
        "        shape: tensor shape of smaller tensor\n",
        "        num_dims: number of dimensions in smaller tensor\n",
        "\n",
        "     Returns:\n",
        "        None (Fills in out_index)\n",
        "    '''\n",
        "    for i in range(num_dims):\n",
        "        if shape[i] > 1:\n",
        "            out_index[i] = big_index[i + (num_dims_big - num_dims)]\n",
        "        else:\n",
        "            out_index[i] = 0\n"
      ],
      "metadata": {
        "id": "2iBo38geZqsV"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@cuda.jit('void(float32[:], int32[:], int32[:], float32[:], int32[:], int32[:], float32[:], int32[:], int32[:])')\n",
        "def simple_matmul(out, out_shape, out_strides, a, a_shape, a_strides, b, b_shape, b_strides):\n",
        "  '''\n",
        "  a: M X H\n",
        "  b: H x N\n",
        "  c: M x N\n",
        "  all shapes and strides are 2-tuples\n",
        "  '''\n",
        "  sz = np.dtype(np.float32).itemsize\n",
        "  x = cuda.threadIdx.x\n",
        "  bx = cuda.blockIdx.x\n",
        "  bd = cuda.blockDim.x\n",
        "\n",
        "  out_ordinal = bd * bx + x\n",
        "  out_index = [0]*2\n",
        "  to_index(out_ordinal, out_shape, out_index, 2)\n",
        "  if out_index[0] >= out_shape[0] or out_index[1] >= out_shape[1]:\n",
        "    return\n",
        "  out_pos = int(index_to_position(out_index, out_strides, 2))\n",
        "  out[out_pos//sz] = 0\n",
        "  for k in range(a_shape[1]):\n",
        "    a_index = [out_index[0], k]\n",
        "    b_index = [k, out_index[1]]\n",
        "    a_pos = int(index_to_position(a_index, a_strides, 2))\n",
        "    b_pos = int(index_to_position(b_index, b_strides, 2))\n",
        "    out[out_pos//sz] += a[a_pos//sz] * b[b_pos//sz]\n",
        "  cuda.syncthreads()"
      ],
      "metadata": {
        "id": "e0l_YJbempgR"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@cuda.jit('void(float32[:], int32[:], int32[:], float32[:], int32[:], int32[:], float32[:], int32[:], int32[:])')\n",
        "def simple_batch_matmul(out, out_shape, out_strides, a, a_shape, a_strides, b, b_shape, b_strides):\n",
        "  '''\n",
        "  a: B X M X H\n",
        "  b: B X H x N\n",
        "  c: B X M x N\n",
        "  all shapes and strides are 3-tuples\n",
        "  '''\n",
        "  ## Each block is threads_per_block x threads_per_block\n",
        "  ## Each grid is ceil(M/threads_per_block) x ceil(N/threads_per_block) x B\n",
        "  sz = 4 ## float size. Don't know how to do sizeof(float)\n",
        "  x = cuda.threadIdx.x\n",
        "  bx = cuda.blockIdx.x\n",
        "  bd = cuda.blockDim.x\n",
        "\n",
        "  out_ordinal = bd * bx + x\n",
        "  out_index = [0]*2\n",
        "  to_index(out_ordinal, out_shape, out_index, 2)\n",
        "  if out_index[0] >= out_shape[0] or out_index[1] >= out_shape[1]:\n",
        "    return\n",
        "  out_pos = int(index_to_position(out_index, out_strides, 2))\n",
        "  out[out_pos//sz] = 0\n",
        "  for k in range(a_shape[1]):\n",
        "    a_index = [out_index[0], k]\n",
        "    b_index = [k, out_index[1]]\n",
        "    a_pos = int(index_to_position(a_index, a_strides, 2))\n",
        "    b_pos = int(index_to_position(b_index, b_strides, 2))\n",
        "    out[out_pos//sz] += a[a_pos//sz] * b[b_pos//sz]\n",
        "  cuda.syncthreads()"
      ],
      "metadata": {
        "id": "7YN5WB5BDhBe"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.random.randn(3, 4).astype(np.float32)\n",
        "B = np.random.randn(4, 2).astype(np.float32)\n",
        "C = np.zeros((3, 2), dtype=np.float32)\n",
        "gridDim = 2\n",
        "blockDim = 4\n",
        "C_np = A @ B\n",
        "simple_matmul[gridDim, blockDim](C.reshape(-1), C.shape, C.strides, A.reshape(-1), A.shape, A.strides, B.reshape(-1), B.shape, B.strides)\n",
        "\n",
        "assert np.allclose(C, C_np)"
      ],
      "metadata": {
        "id": "bviPhF1Yr4Su"
      },
      "execution_count": 164,
      "outputs": []
    }
  ]
}