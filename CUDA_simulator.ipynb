{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgyGZ+GdM7jBpmkkwxqrWo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshulsawant/llm-systems/blob/main/CUDA_simulator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ovnlWerSVhG5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['NUMBA_ENABLE_CUDASIM'] = '1'\n",
        "import numba\n",
        "import numpy as np\n",
        "from numba import cuda"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@cuda.jit\n",
        "def vec_add(A, B, n, out):\n",
        "    x = cuda.threadIdx.x\n",
        "    bx = cuda.blockIdx.x\n",
        "    bdx = cuda.blockDim.x\n",
        "    i = bx * bdx + x\n",
        "    if i < n:\n",
        "      out[i] = A[i] + B[i]"
      ],
      "metadata": {
        "id": "vVDLI42_VpCe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 14\n",
        "A = np.arange(n)\n",
        "B = np.ones_like(A)\n",
        "C = np.zeros_like(A)"
      ],
      "metadata": {
        "id": "8p4N4kZlWYMP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "griddim = 4\n",
        "blockdim = 4\n",
        "vec_add[griddim, blockdim](A, B, n, C)\n",
        "C"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1q6ukcbXk-M",
        "outputId": "972baa4a-9c44-4cac-93e8-de779b043ca4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@cuda.jit(device=True)\n",
        "def index_to_position(index, strides, num_dims):\n",
        "    '''\n",
        "     Converts a multidimensional tensor index into a single-dimensional position in storage\n",
        "     based on strides.\n",
        "     Args:\n",
        "        index: index tuple of ints\n",
        "        strides: tensor strides\n",
        "        num_dims: number of dimensions in the tensor, e.g. shape/strides of [2, 3, 4] has 3 dimensions\n",
        "\n",
        "     Returns:\n",
        "        int - position in storage\n",
        "    '''\n",
        "    position = 0;\n",
        "    for i in range(num_dims):\n",
        "        position += index[i] * strides[i];\n",
        "    return position;\n",
        "\n",
        "@cuda.jit(device=True)\n",
        "def to_index(ordinal, shape, out_index, num_dims):\n",
        "    '''\n",
        "     Convert an ordinal to an index in the shape. Should ensure that enumerating position 0 ... size of\n",
        "     a tensor produces every index exactly once. It may not be the inverse of index_to_position.\n",
        "     Args:\n",
        "        ordinal: ordinal position to convert\n",
        "        shape: tensor shape\n",
        "        out_index: return index corresponding to position\n",
        "        num_dims: number of dimensions in the tensor\n",
        "\n",
        "     Returns:\n",
        "        None (Fills in out_index)\n",
        "    '''\n",
        "    cur_ord = ordinal;\n",
        "    for i in reversed(range(num_dims)):\n",
        "        sh = shape[i];\n",
        "        out_index[i] = cur_ord % sh;\n",
        "        cur_ord //= sh;\n",
        "\n",
        "@cuda.jit(device=True)\n",
        "def broadcast_index(big_index, big_shape, shape, out_index, num_dims_big, num_dims):\n",
        "    '''\n",
        "     Convert a big_index into big_shape to a smaller out_index into shape following broadcasting rules.\n",
        "     In this case it may be larger or with more dimensions than the shape given.\n",
        "     Additional dimensions may need to be mapped to 0 or removed.\n",
        "\n",
        "     Args:\n",
        "        big_index: multidimensional index of bigger tensor\n",
        "        big_shape: tensor shape of bigger tensor\n",
        "        nums_big_dims: number of dimensions in bigger tensor\n",
        "        out_index: multidimensional index of smaller tensor\n",
        "        shape: tensor shape of smaller tensor\n",
        "        num_dims: number of dimensions in smaller tensor\n",
        "\n",
        "     Returns:\n",
        "        None (Fills in out_index)\n",
        "    '''\n",
        "    for i in range(num_dims):\n",
        "        if shape[i] > 1:\n",
        "            out_index[i] = big_index[i + (num_dims_big - num_dims)]\n",
        "        else:\n",
        "            out_index[i] = 0\n",
        ""
      ],
      "metadata": {
        "id": "2iBo38geZqsV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@cuda.jit('void(float32[:], int32[:], int32[:], float32[:], int32[:], int32[:], float32[:], int32[:], int32[:])')\n",
        "def simple_matmul(out, out_shape, out_strides, a, a_shape, a_strides, b, b_shape, b_strides):\n",
        "  '''\n",
        "  a: M X H\n",
        "  b: H x N\n",
        "  c: M x N\n",
        "  all shapes and strides are 2-tuples\n",
        "  '''\n",
        "  sz = np.dtype(np.float32).itemsize\n",
        "  x = cuda.threadIdx.x\n",
        "  bx = cuda.blockIdx.x\n",
        "  bd = cuda.blockDim.x\n",
        "\n",
        "  out_ordinal = bd * bx + x\n",
        "  out_index = [0]*2\n",
        "  to_index(out_ordinal, out_shape, out_index, 2)\n",
        "  if out_index[0] >= out_shape[0] or out_index[1] >= out_shape[1]:\n",
        "    return\n",
        "  out_pos = int(index_to_position(out_index, out_strides, 2))\n",
        "  out[out_pos//sz] = 0\n",
        "  for k in range(a_shape[1]):\n",
        "    a_index = [out_index[0], k]\n",
        "    b_index = [k, out_index[1]]\n",
        "    a_pos = int(index_to_position(a_index, a_strides, 2))\n",
        "    b_pos = int(index_to_position(b_index, b_strides, 2))\n",
        "    out[out_pos//sz] += a[a_pos//sz] * b[b_pos//sz]\n",
        "  cuda.syncthreads()"
      ],
      "metadata": {
        "id": "e0l_YJbempgR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@cuda.jit('void(float32[:], int32[:], int32[:], float32[:], int32[:], int32[:], float32[:], int32[:], int32[:], int32)')\n",
        "def tiled_batch_matmul(out, out_shape, out_strides, a, a_shape, a_strides, b, b_shape, b_strides):\n",
        "  '''\n",
        "  a: B X M X H\n",
        "  b: B X H x N\n",
        "  c: B X M x N\n",
        "  all shapes and strides are 3-tuples\n",
        "  '''\n",
        "  sz = np.dtype(np.float32).itemsize\n",
        "  num_dims = 3\n",
        "\n",
        "  ## assert cuda.blockDim.x == cuda.blockDim.y\n",
        "\n",
        "\n",
        "  tile_size = cuda.blockDim.x\n",
        "  _a = cuda.shared.array((tile_size, tile_size), dtype=np.float32)\n",
        "  _b = cuda.shared.array((tile_size, tile_size), dtype=np.float32)\n",
        "  ## Each block is threads_per_block x threads_per_block\n",
        "  ## Each grid is ceil(M/threads_per_block) x ceil(N/threads_per_block) x B\n",
        "\n",
        "  batch = cuda.blockIdx.z\n",
        "  if batch >= out_shape[0]:\n",
        "    return\n",
        "\n",
        "  tx = cuda.threadIdx.x\n",
        "  ty = cuda.threadIdx.y\n",
        "\n",
        "  out_x = cuda.blockIdx.x * tile_size + tx\n",
        "  out_y = cuda.blockIdx.y * tile_size + ty\n",
        "\n",
        "  out_index = [batch, out_x, out_y]\n",
        "  out_value = 0\n",
        "  for k in range(0, a_shape[2], tile_size):\n",
        "    a_index = [batch, out_index[1], k + ty]\n",
        "    if a_index[1] < a_shape[1] and a_index[2] < a_shape[2]:\n",
        "      a_pos = int(index_to_position(a_index, a_strides, num_dims))\n",
        "      _a[tx, ty] = a[a_pos//sz]\n",
        "    else:\n",
        "      _a[tx, ty] = 0.0\n",
        "    b_index = [batch, k + tx, out_index[2]]\n",
        "    if b_index[2] < b_shape[2] and b_index[1] < b_shape[1]:\n",
        "      b_pos = int(index_to_position(b_index, b_strides, num_dims))\n",
        "      _b[tx, ty] = b[b_pos//sz]\n",
        "    else:\n",
        "      _b[tx, ty] = 0.0\n",
        "    cuda.syncthreads()\n",
        "    for i in range(tile_size):\n",
        "      if k + i < a_shape[2] and k + i < b_shape[1]:\n",
        "        out_value += _a[tx, i] * _b[i, ty]\n",
        "    cuda.syncthreads()\n",
        "  if out_index[1] >= out_shape[1] or out_index[2] >= out_shape[2]:\n",
        "    return\n",
        "  out_pos = int(index_to_position(out_index, out_strides, num_dims))\n",
        "  out[out_pos//sz] = out_value"
      ],
      "metadata": {
        "id": "7YN5WB5BDhBe"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 3\n",
        "M = 3\n",
        "N = 2\n",
        "H = 4\n",
        "threads_per_block = 2\n",
        "## A = np.random.randn(batch_size, M, H).astype(np.float32)\n",
        "## B = np.random.randn(batch_size, H, N).astype(np.float32)\n",
        "A = np.arange(batch_size * M * H).reshape((batch_size, M, H)).astype(np.float32)\n",
        "B = np.arange(batch_size * H * N).reshape((batch_size, H, N)).astype(np.float32)\n",
        "gridDim = ((M + threads_per_block - 1)//threads_per_block, (N + threads_per_block - 1)//threads_per_block, batch_size)\n",
        "blockDim = (threads_per_block, threads_per_block)\n",
        "C_np = A @ B\n",
        "C = np.zeros_like(C_np)\n",
        "grid_dim = (int(), int(), 1)\n",
        "tiled_batch_matmul[gridDim, blockDim](C.reshape(-1), C.shape, C.strides, A.reshape(-1), A.shape, A.strides, B.reshape(-1), B.shape, B.strides)\n",
        "\n",
        "assert np.allclose(C, C_np)"
      ],
      "metadata": {
        "id": "bviPhF1Yr4Su"
      },
      "execution_count": 114,
      "outputs": []
    }
  ]
}